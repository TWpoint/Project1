{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88bf6bd0-3dd2-4711-9cce-b6dbb463e05d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 1: Understand body language by gesture recognition with fully connected neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdebc998-13f5-4195-a67b-df152735d5b9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Do literature search on gesture recognition and its application in Human-Robot Interaction. Summarize what you have learned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d7314-8adc-4e9e-878f-1cf6b6d98a2d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Referring to the previous example about building a neural network based classifier, use what you have learned to read the code for gesture classification below and design your own network architecture using fully connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7337f2e1-9947-4985-a97b-c807c311e28f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Run the model. Analyse and comment on the performance of your model based on fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "188f9266-fc0b-4001-84ba-73e7804606e6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import torch.utils.data as utils_data\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d0830f-0431-4d27-aed5-5fa3156d6091",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1) data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926df832-9898-4e91-8beb-d0790da1bf0c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define a function to preprocess the images including resizing and binaryzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd1d99d8-da0f-4305-8fbd-1e346abc560b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def processSkinImage(filePath, resize_HW=48):\n",
    "    import cv2\n",
    "    # step 1\n",
    "    # read the image\n",
    "    original = cv2.imread(filename=filePath)\n",
    "\n",
    "    # step 2\n",
    "    # resize the image to\n",
    "    image_resized = cv2.resize(original, (resize_HW, resize_HW))\n",
    "\n",
    "    # step 3\n",
    "    # convert the image from rgb to YCbCr\n",
    "    image_ycbcr = cv2.cvtColor(image_resized, cv2.COLOR_BGR2YCR_CB)\n",
    "\n",
    "    # step 4\n",
    "    # get the central color of the image\n",
    "    # expected the hand to be in the central of the image\n",
    "    Cb_center_color = image_ycbcr[int(resize_HW / 2), int(resize_HW / 2), 1]\n",
    "    Cr_center_color = image_ycbcr[int(resize_HW / 2), int(resize_HW / 2), 2]\n",
    "    # set the range\n",
    "    Cb_Difference = 15\n",
    "    Cr_Difference = 10\n",
    "\n",
    "    # step 5\n",
    "    # detect skin pixels\n",
    "    Cb = image_ycbcr[:, :, 1]\n",
    "    Cr = image_ycbcr[:, :, 2]\n",
    "    index = np.where((Cb >= Cb_center_color - Cb_Difference) & (Cb <= Cb_center_color + Cb_Difference)\n",
    "                     & (Cr >= Cr_center_color - Cr_Difference) & (Cr <= Cr_center_color + Cr_Difference))\n",
    "\n",
    "    # Mark detected pixels and output\n",
    "    image_output = np.zeros((resize_HW, resize_HW))\n",
    "    image_output[index] = 255\n",
    "\n",
    "    # show image\n",
    "    # cv2.imshow(\"\", image_output)\n",
    "    # cv2.waitKey(0)\n",
    "    return image_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf454b1-4248-4a3e-9bf8-9304d64131c4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Deal with all the images using the function defined above.\n",
    "The processed data is stored in a new folder 'dataset_processed'.\n",
    "\n",
    "Generate labels for each class. (class 0, 1, ..., num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a67e88cb-3dc7-4bfe-935f-5cba75a782d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = './dataset/images'\n",
    "path_processed = './dataset_processed/images'\n",
    "\n",
    "a = cv2.imread(path)\n",
    "\n",
    "# -------------------images processing--------------\n",
    "for mainDir, subDir, fileList in os.walk(path):\n",
    "    for file in fileList:\n",
    "        currentPath = os.path.join(mainDir, file)\n",
    "        if file != '.DS_Store':\n",
    "            processedImage = processSkinImage(currentPath)\n",
    "\n",
    "            new_mainDir = path_processed + mainDir.split(path)[-1]\n",
    "            if not os.path.exists(new_mainDir):\n",
    "                os.makedirs(new_mainDir)\n",
    "            cv2.imwrite(os.path.join(new_mainDir, file), processedImage)\n",
    "\n",
    "# -----------------label generation----------------\n",
    "label_path = './dataset_processed/labels'\n",
    "if not os.path.exists(label_path):\n",
    "    os.makedirs(label_path)\n",
    "\n",
    "files = os.listdir(path)\n",
    "index = 0\n",
    "for i, file in enumerate(files):\n",
    "    if file != '.DS_Store':\n",
    "        subclass_label_path = os.path.join(label_path, file + '.txt')\n",
    "        with open(subclass_label_path, 'w') as f:\n",
    "            f.write('#label\\n')\n",
    "        for _ in range(len(os.listdir(os.path.join(path_processed, file)))):\n",
    "            with open(subclass_label_path, 'a') as f:\n",
    "                f.write('{:d}\\n'.format(index))\n",
    "        index = index + 1\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac1d92-e269-4297-8b52-6be08c37e503",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2) load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e44415d-726c-401c-9a78-5dcd2c140ae2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78, 48, 48)\n"
     ]
    }
   ],
   "source": [
    "Image = []\n",
    "path_images = './dataset_processed/images'\n",
    "for mainDir, subDir, fileList in os.walk(path_images):\n",
    "    for file in fileList:\n",
    "        currentPath = os.path.join(mainDir, file)\n",
    "        Image.append(cv2.imread(currentPath)[:, :, 0])\n",
    "Image = np.array(Image)\n",
    "dataset_size, H, W = Image.shape\n",
    "print(Image.shape)\n",
    "# for FCNN model, the image need to be stretched into one dimension: (b, h, w)->(b, h*w)\n",
    "Image = Image.reshape(dataset_size, -1)\n",
    "\n",
    "Label = []\n",
    "path_labels = './dataset_processed/labels'\n",
    "for file in os.listdir(path_labels):\n",
    "    Label.append(np.loadtxt(os.path.join(path_labels, file)))\n",
    "Label = np.array(list(itertools.chain.from_iterable(Label)))\n",
    "num_classes = int(np.max(Label)) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e71e6c-b618-4a68-90da-64ab0720edab",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3) build your own neural network based on fully connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c77568-a973-444d-95c8-b9fb70190a6f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Design the neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbbd1a27-4d24-4511-8577-1737a8238616",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FCNNModel(nn.Module):\n",
    "    def __init__(self, input_layer_size, hidden_layer_size, num_classes):\n",
    "        super(FCNNModel, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(input_layer_size, hidden_layer_size), nn.BatchNorm1d(hidden_layer_size),\n",
    "                                    nn.ReLU(True))\n",
    "        self.layer2 = nn.Sequential(nn.Linear(hidden_layer_size, hidden_layer_size), nn.BatchNorm1d(hidden_layer_size),\n",
    "                                    nn.ReLU(True))\n",
    "        self.layer3 = nn.Sequential(nn.Linear(hidden_layer_size, num_classes))\n",
    "\n",
    "        # code by yourself\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        out = self.layer3(x)\n",
    "        return out\n",
    "        # code by yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0662eb3b-c72d-4331-8c4f-8190805692c0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "instantiate your model, set a optimizer and define a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1354717-c8e5-4160-a15a-cba6d2669912",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = FCNNModel(input_layer_size=H * W, hidden_layer_size=int(H * W / 2), num_classes=num_classes)\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd1f77-bf0f-43be-bbfd-dfc3dfca6d57",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4) train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f57afac-7916-4851-89be-3e6f54c4e34d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Encapsulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "637ac95d-107a-4f69-b570-2d38352d3cfc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ready!\n"
     ]
    }
   ],
   "source": [
    "dataset = utils_data.TensorDataset(torch.Tensor(Image), torch.LongTensor(Label))\n",
    "split_ratio = 0.8\n",
    "train_size = int(split_ratio * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "train_set, test_set = utils_data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = utils_data.DataLoader(dataset=train_set, batch_size=8, shuffle=True)\n",
    "test_loader = utils_data.DataLoader(dataset=test_set, batch_size=8, shuffle=True)\n",
    "print('Data is ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea0608-ff4d-4da7-94b1-c1d62061d87b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following is the training and testing process in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0\ttrain loss=1.207272\ttrain accuracy=0.500\ttest accuracy=0.750\n",
      "epoch=1\ttrain loss=0.537193\ttrain accuracy=0.758\ttest accuracy=0.812\n",
      "epoch=2\ttrain loss=0.420058\ttrain accuracy=0.823\ttest accuracy=0.812\n",
      "epoch=3\ttrain loss=0.410733\ttrain accuracy=0.871\ttest accuracy=0.750\n",
      "epoch=4\ttrain loss=0.146023\ttrain accuracy=0.968\ttest accuracy=0.688\n",
      "epoch=5\ttrain loss=0.133185\ttrain accuracy=0.935\ttest accuracy=0.750\n",
      "epoch=6\ttrain loss=0.233237\ttrain accuracy=0.919\ttest accuracy=0.750\n",
      "epoch=7\ttrain loss=0.117188\ttrain accuracy=0.952\ttest accuracy=0.875\n",
      "epoch=8\ttrain loss=0.541779\ttrain accuracy=0.790\ttest accuracy=0.750\n",
      "epoch=9\ttrain loss=0.151686\ttrain accuracy=0.903\ttest accuracy=0.812\n",
      "epoch=10\ttrain loss=0.205728\ttrain accuracy=0.903\ttest accuracy=0.750\n",
      "epoch=11\ttrain loss=0.136889\ttrain accuracy=0.952\ttest accuracy=0.750\n",
      "epoch=12\ttrain loss=0.160429\ttrain accuracy=0.952\ttest accuracy=0.688\n",
      "epoch=13\ttrain loss=0.095766\ttrain accuracy=0.968\ttest accuracy=0.688\n",
      "epoch=14\ttrain loss=0.053125\ttrain accuracy=1.000\ttest accuracy=0.750\n",
      "epoch=15\ttrain loss=0.077346\ttrain accuracy=0.968\ttest accuracy=0.812\n",
      "epoch=16\ttrain loss=0.072563\ttrain accuracy=0.984\ttest accuracy=0.812\n",
      "epoch=17\ttrain loss=0.036168\ttrain accuracy=0.984\ttest accuracy=0.812\n",
      "epoch=18\ttrain loss=0.040139\ttrain accuracy=0.984\ttest accuracy=0.750\n",
      "epoch=19\ttrain loss=0.038068\ttrain accuracy=0.984\ttest accuracy=0.750\n",
      "epoch=20\ttrain loss=0.035933\ttrain accuracy=0.984\ttest accuracy=0.812\n",
      "epoch=21\ttrain loss=0.077881\ttrain accuracy=0.968\ttest accuracy=0.812\n",
      "epoch=22\ttrain loss=0.062709\ttrain accuracy=0.968\ttest accuracy=0.750\n",
      "epoch=23\ttrain loss=0.068299\ttrain accuracy=0.968\ttest accuracy=0.688\n",
      "epoch=24\ttrain loss=0.024572\ttrain accuracy=1.000\ttest accuracy=0.750\n",
      "epoch=25\ttrain loss=0.125712\ttrain accuracy=0.968\ttest accuracy=0.688\n",
      "epoch=26\ttrain loss=0.050281\ttrain accuracy=0.984\ttest accuracy=0.812\n",
      "epoch=27\ttrain loss=0.223981\ttrain accuracy=0.919\ttest accuracy=0.812\n",
      "epoch=28\ttrain loss=0.140960\ttrain accuracy=0.952\ttest accuracy=0.812\n",
      "epoch=29\ttrain loss=0.109542\ttrain accuracy=0.952\ttest accuracy=0.875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     12\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     13\u001B[0m batch_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 14\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m batch_loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# train accuracy\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/Project1/lib/python3.10/site-packages/torch/optim/optimizer.py:113\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    111\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[0;32m--> 113\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/Project1/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/Project1/lib/python3.10/site-packages/torch/optim/adam.py:157\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    153\u001B[0m                 max_exp_avg_sqs\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_exp_avg_sq\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m    155\u001B[0m             state_steps\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m--> 157\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m         \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m         \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m         \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m         \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    162\u001B[0m \u001B[43m         \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[43m         \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m         \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m         \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    166\u001B[0m \u001B[43m         \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[43m         \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[43m         \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[43m         \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[43m         \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[43m         \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/.conda/envs/Project1/lib/python3.10/site-packages/torch/optim/adam.py:213\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    211\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 213\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    214\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    216\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    217\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    218\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    219\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    220\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    222\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    223\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    224\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    226\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/Project1/lib/python3.10/site-packages/torch/optim/adam.py:262\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001B[0m\n\u001B[1;32m    259\u001B[0m     grad \u001B[38;5;241m=\u001B[39m grad\u001B[38;5;241m.\u001B[39madd(param, alpha\u001B[38;5;241m=\u001B[39mweight_decay)\n\u001B[1;32m    261\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[0;32m--> 262\u001B[0m \u001B[43mexp_avg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    263\u001B[0m exp_avg_sq\u001B[38;5;241m.\u001B[39mmul_(beta2)\u001B[38;5;241m.\u001B[39maddcmul_(grad, grad\u001B[38;5;241m.\u001B[39mconj(), value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n\u001B[1;32m    265\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m capturable:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "for epoch in range(500):\n",
    "    running_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    for step, (batch_image, batch_label) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        # if torch.cuda.is_available():\n",
    "        #     batch_image, batch_label = batch_image.cuda(), batch_label.cuda()\n",
    "        batch_output = model(batch_image)\n",
    "        batch_loss = loss_func(batch_output, batch_label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += batch_loss.item()\n",
    "\n",
    "        # train accuracy\n",
    "        _, train_predicted = torch.max(batch_output.data, 1)\n",
    "        train_acc += (train_predicted == batch_label).sum().item()\n",
    "\n",
    "    train_acc /= train_size\n",
    "    running_loss /= (step + 1)\n",
    "\n",
    "    # ----------test----------\n",
    "    model.eval()\n",
    "    test_acc = 0.0\n",
    "    for test_image, test_label in test_loader:\n",
    "        test_output = model(test_image)\n",
    "        _, predicted = torch.max(test_output.data, 1)\n",
    "        test_acc += (predicted == test_label).sum().item()\n",
    "    test_acc /= test_size\n",
    "\n",
    "    print('epoch={:d}\\ttrain loss={:.6f}\\ttrain accuracy={:.3f}\\ttest accuracy={:.3f}'.format(\n",
    "        epoch, running_loss, train_acc, test_acc))\n",
    "\n",
    "    if test_acc >= best_accuracy:\n",
    "        torch.save(model.state_dict(), './trained_models/FCNN_model.pkl')\n",
    "        best_accuracy = test_acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}